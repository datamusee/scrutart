# ğŸš¦ Gestion des Limites de l'API Wikidata

## ProblÃ¨me

Wikidata impose des limites strictes sur les requÃªtes SPARQL :
- **60 requÃªtes par minute** maximum
- **Timeout aprÃ¨s 60 secondes** par requÃªte
- **429 Too Many Requests** si vous dÃ©passez la limite

## Erreurs Communes

```
Erreur SPARQL: HTTPSConnectionPool: Read timed out
Erreur SPARQL: 429 Client Error: Too Many Requests
```

## Solutions ImplÃ©mentÃ©es

### 1. DÃ©lai Entre les RequÃªtes

**Dans `property_matcher.py` :**
```python
REQUEST_DELAY = 1.5  # 1.5 secondes entre chaque requÃªte
```

Cela limite Ã  ~40 requÃªtes/minute (marge de sÃ©curitÃ©).

### 2. Retry Automatique

```python
MAX_RETRIES = 3

# Si erreur 429 : attente progressive (5s, 10s, 15s)
# Si timeout : attente de 2s, 4s, 6s
```

### 3. Timeout RÃ©duit

```python
REQUEST_TIMEOUT = 15  # 15 secondes au lieu de 30
```

Les requÃªtes complexes qui timeout sont probablement trop lourdes.

### 4. Limitation des PropriÃ©tÃ©s

**Dans les tests et l'application :**
- Limiter les mots-clÃ©s Ã  5 maximum
- Traiter les propriÃ©tÃ©s essentielles en prioritÃ©

## Bonnes Pratiques

### 1. Optimiser les RequÃªtes SPARQL

**âŒ MAUVAIS - Trop large :**
```sparql
SELECT * WHERE {
  ?item ?p ?o .
  FILTER(CONTAINS(?o, "mot"))
}
```

**âœ… BON - FiltrÃ© tÃ´t :**
```sparql
SELECT ?item ?itemLabel WHERE {
  ?item wdt:P31 wd:Q5 .  # Filtre type d'abord
  ?item rdfs:label "Nom exact"@fr .
  SERVICE wikibase:label { bd:serviceParam wikibase:language "fr" . }
}
LIMIT 5
```

### 2. Traiter Par Lots

Si vous avez beaucoup de propriÃ©tÃ©s :
```python
# Diviser en groupes
keywords_batch1 = keywords[:5]
keywords_batch2 = keywords[5:10]

# Traiter avec pause entre les lots
results1 = process_batch(keywords_batch1)
time.sleep(5)  # Pause de 5s entre les lots
results2 = process_batch(keywords_batch2)
```

### 3. Prioriser les PropriÃ©tÃ©s

Ordre de traitement recommandÃ© :
1. **CrÃ©ateur** (1 requÃªte)
2. **Personnes reprÃ©sentÃ©es** (1-3 requÃªtes)
3. **Top 3-5 mots-clÃ©s** (3-5 requÃªtes)
4. **MatÃ©riau** (1 requÃªte)
5. **Collection** (1 requÃªte si nÃ©cessaire)

**Total : 7-11 requÃªtes** par peinture â†’ Safe

### 4. Ã‰viter les RequÃªtes en ParallÃ¨le

**âŒ Ne PAS faire :**
```python
# Toutes les requÃªtes en mÃªme temps
with ThreadPoolExecutor() as executor:
    futures = [executor.submit(search, kw) for kw in keywords]
```

**âœ… Faire :**
```python
# Une aprÃ¨s l'autre avec dÃ©lai
for keyword in keywords:
    result = search(keyword)
    time.sleep(1.5)
```

### 5. Utiliser le Cache

**Important :** Les mÃªmes requÃªtes peuvent Ãªtre faites plusieurs fois.

ImplÃ©menter un cache simple :
```python
cache = {}

def search_with_cache(term):
    if term in cache:
        return cache[term]
    
    result = search(term)
    cache[term] = result
    return result
```

## Configuration RecommandÃ©e

### Pour les Tests

```python
# test_property_matching.py
extracted_data = {
    'creator': '...',
    'keywords': [...][:3],  # Limiter Ã  3
    'depicted_persons': [...][: 2],  # Limiter Ã  2
}
```

### Pour la Production

```python
# property_matcher.py
class PropertyMatcher:
    REQUEST_DELAY = 1.5  # Ajuster selon vos besoins
    MAX_RETRIES = 3
    REQUEST_TIMEOUT = 15
    MAX_KEYWORDS = 5  # Limiter le nombre de mots-clÃ©s
```

## Monitoring

### Compter les RequÃªtes

Ajouter un compteur :
```python
class PropertyMatcher:
    def __init__(self):
        self.request_count = 0
        self.start_time = time.time()
    
    def _execute_sparql(self, query):
        self.request_count += 1
        
        # VÃ©rifier le taux
        elapsed = time.time() - self.start_time
        rate = self.request_count / (elapsed / 60)  # requÃªtes/minute
        
        if rate > 50:
            print(f"âš ï¸  Taux Ã©levÃ©: {rate:.1f} req/min")
            time.sleep(2)  # Ralentir
        
        # ... rest of method
```

## Alternatives

### 1. Utiliser l'API Wikidata Entity Search

Pour les recherches simples, utiliser :
```python
# Au lieu de SPARQL
endpoint = "https://www.wikidata.org/w/api.php"
params = {
    'action': 'wbsearchentities',
    'format': 'json',
    'language': 'fr',
    'search': term,
    'limit': 5
}
```

**Avantage :** Plus rapide, moins de limitations.

### 2. Batch Processing

Traiter plusieurs Ã©lÃ©ments dans une seule requÃªte SPARQL (quand possible) :
```sparql
SELECT ?item ?itemLabel WHERE {
  VALUES ?label { "mot1"@fr "mot2"@fr "mot3"@fr }
  ?item rdfs:label ?label .
}
```

### 3. Utiliser un Service Tiers

- **Wikidata Query Service UI** : Pour tester les requÃªtes
- **SPARQL Proxy** : Pour mettre en cache les rÃ©sultats

## Erreurs et Solutions

| Erreur | Cause | Solution |
|--------|-------|----------|
| 429 Too Many Requests | Trop de requÃªtes/minute | Augmenter REQUEST_DELAY Ã  2s |
| Read timeout | RequÃªte trop complexe | Simplifier la requÃªte SPARQL |
| Connection refused | Service surchargÃ© | RÃ©essayer plus tard |
| Empty results | RequÃªte trop stricte | Assouplir les filtres |

## Checklist Avant de Lancer

- [ ] REQUEST_DELAY >= 1.5 secondes
- [ ] Mots-clÃ©s limitÃ©s Ã  5 maximum
- [ ] RequÃªtes SPARQL optimisÃ©es (LIMIT, filtres)
- [ ] Retry logic implÃ©mentÃ©
- [ ] Timeout raisonnable (15-20s)
- [ ] Messages de progression pour l'utilisateur
- [ ] Gestion d'erreurs gracieuse

## Test Progressif

```bash
# Test 1 : Une seule propriÃ©tÃ©
python -c "from property_matcher import PropertyMatcher; \
           m = PropertyMatcher(); \
           print(m.search_person('Ã‰douard Toudouze', '1848', '1907'))"

# Test 2 : Quelques propriÃ©tÃ©s
python test_property_matching.py

# Test 3 : Charge complÃ¨te
# Seulement si les tests prÃ©cÃ©dents passent
```

## RÃ©sumÃ©

**RÃ¨gles d'or :**
1. â±ï¸  **1.5s minimum** entre chaque requÃªte
2. ğŸ”¢ **5 mots-clÃ©s max** par traitement
3. ğŸ”„ **3 retries** en cas d'erreur
4. â° **15s timeout** par requÃªte
5. ğŸ“Š **Optimiser** les requÃªtes SPARQL
6. ğŸ’¾ **Cacher** les rÃ©sultats quand possible

Avec ces rÃ¨gles, vous restez largement sous la limite de 60 req/min ! âœ…
